"use strict";(self.webpackChunkApache_Cloudberry_Incubating_website=self.webpackChunkApache_Cloudberry_Incubating_website||[]).push([[21769],{55152:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var a=n(85893),s=n(11151);const i={title:"Handle Data Loading Errors"},t="Handle Data Loading Errors",o={id:"data-loading/handle-data-errors",title:"Handle Data Loading Errors",description:"Real-world data is often imperfect, containing formatting errors, missing values, or inconsistent data types. Apache Cloudberry provides robust error handling mechanisms that allow you to load correctly formatted data while isolating and managing problematic rows, ensuring your ETL processes are resilient and reliable.",source:"@site/versioned_docs/version-2.x/data-loading/handle-data-errors.md",sourceDirName:"data-loading",slug:"/data-loading/handle-data-errors",permalink:"/docs/data-loading/handle-data-errors",draft:!1,unlisted:!1,editUrl:"https://github.com/apache/cloudberry-site/edit/main/versioned_docs/version-2.x/data-loading/handle-data-errors.md",tags:[],version:"2.x",lastUpdatedBy:"Dianjin Wang",lastUpdatedAt:1758251752,formattedLastUpdatedAt:"Sep 19, 2025",frontMatter:{title:"Handle Data Loading Errors"},sidebar:"docsbars",previous:{title:"Load Data from Kafka Using Kafka FDW",permalink:"/docs/data-loading/load-data-from-kafka-using-fdw"},next:{title:"Create and Manage Database",permalink:"/docs/operate-with-data/operate-with-db-objects/create-and-manage-database"}},l={},d=[{value:"Error handling modes",id:"error-handling-modes",level:2},{value:"Single row error isolation",id:"single-row-error-isolation",level:2},{value:"Basic error isolation",id:"basic-error-isolation",level:3},{value:"Percentage-based limits",id:"percentage-based-limits",level:3},{value:"Error logging",id:"error-logging",level:2},{value:"Enable error logging",id:"enable-error-logging",level:3},{value:"Persistent error logging",id:"persistent-error-logging",level:3},{value:"View error information",id:"view-error-information",level:2},{value:"Query error logs",id:"query-error-logs",level:3},{value:"Error log table structure",id:"error-log-table-structure",level:3},{value:"Example error analysis",id:"example-error-analysis",level:3},{value:"Common data errors and solutions",id:"common-data-errors-and-solutions",level:2},{value:"Data type conversion errors",id:"data-type-conversion-errors",level:3},{value:"Date format issues",id:"date-format-issues",level:3},{value:"Miss or extra columns",id:"miss-or-extra-columns",level:3},{value:"Character encoding issues",id:"character-encoding-issues",level:3},{value:"Error handling strategies",id:"error-handling-strategies",level:2},{value:"Two-phase loading",id:"two-phase-loading",level:3},{value:"Error threshold monitoring",id:"error-threshold-monitoring",level:3},{value:"Best practices",id:"best-practices",level:2},{value:"Design for errors",id:"design-for-errors",level:3},{value:"Error limit guidelines",id:"error-limit-guidelines",level:3},{value:"Operational procedures",id:"operational-procedures",level:3},{value:"Performance considerations",id:"performance-considerations",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"High error rates",id:"high-error-rates",level:3},{value:"Performance issues",id:"performance-issues",level:3},{value:"Learn more",id:"learn-more",level:2}];function c(e){const r={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.h1,{id:"handle-data-loading-errors",children:"Handle Data Loading Errors"}),"\n",(0,a.jsx)(r.p,{children:"Real-world data is often imperfect, containing formatting errors, missing values, or inconsistent data types. Apache Cloudberry provides robust error handling mechanisms that allow you to load correctly formatted data while isolating and managing problematic rows, ensuring your ETL processes are resilient and reliable."}),"\n",(0,a.jsx)(r.p,{children:"By default, if external table data contains any error, the entire load operation fails and no data is loaded. With error handling enabled, you can load valid data and deal with problematic rows separately."}),"\n",(0,a.jsx)(r.h2,{id:"error-handling-modes",children:"Error handling modes"}),"\n",(0,a.jsx)(r.p,{children:"Apache Cloudberry supports two error handling approaches:"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Single row error isolation allows the system to skip individual problematic rows and continue processing the remaining valid data, preventing entire operations from failing due to isolated data quality issues."}),"\n",(0,a.jsx)(r.li,{children:"Error logging functionality captures comprehensive details about problematic data rows, including error descriptions, line numbers, and the actual data that caused the failure, enabling thorough analysis and remediation."}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"single-row-error-isolation",children:"Single row error isolation"}),"\n",(0,a.jsx)(r.h3,{id:"basic-error-isolation",children:"Basic error isolation"}),"\n",(0,a.jsxs)(r.p,{children:["Enable single row error isolation by adding ",(0,a.jsx)(r.code,{children:"SEGMENT REJECT LIMIT"})," to your external table definition:"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE sales_data_with_errors (\n    transaction_id int,\n    product_name text,\n    sale_date date,\n    amount decimal(10,2)\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nSEGMENT REJECT LIMIT 100;\n"})}),"\n",(0,a.jsx)(r.p,{children:"This configuration allows up to 100 rows with errors per segment before the operation fails."}),"\n",(0,a.jsx)(r.h3,{id:"percentage-based-limits",children:"Percentage-based limits"}),"\n",(0,a.jsx)(r.p,{children:"You can also specify error limits as a percentage:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE sales_data (\n    transaction_id int,\n    product_name text, \n    sale_date date,\n    amount decimal(10,2)\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nSEGMENT REJECT LIMIT 5 PERCENT;\n"})}),"\n",(0,a.jsx)(r.p,{children:"This allows up to 5% of rows to contain errors before failing."}),"\n",(0,a.jsx)(r.h2,{id:"error-logging",children:"Error logging"}),"\n",(0,a.jsx)(r.h3,{id:"enable-error-logging",children:"Enable error logging"}),"\n",(0,a.jsxs)(r.p,{children:["Use ",(0,a.jsx)(r.code,{children:"LOG ERRORS"})," to capture detailed information about rejected rows:"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE sales_data_logged (\n    transaction_id int,\n    product_name text,\n    sale_date date,\n    amount decimal(10,2)\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nLOG ERRORS \nSEGMENT REJECT LIMIT 50;\n"})}),"\n",(0,a.jsx)(r.h3,{id:"persistent-error-logging",children:"Persistent error logging"}),"\n",(0,a.jsx)(r.p,{children:"For long-term error analysis, use persistent error logging:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE sales_data_persistent (\n    transaction_id int,\n    product_name text,\n    sale_date date,\n    amount decimal(10,2)\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nLOG ERRORS PERSISTENTLY\nSEGMENT REJECT LIMIT 25;\n"})}),"\n",(0,a.jsx)(r.h2,{id:"view-error-information",children:"View error information"}),"\n",(0,a.jsx)(r.h3,{id:"query-error-logs",children:"Query error logs"}),"\n",(0,a.jsx)(r.p,{children:"When error logging is enabled, Apache Cloudberry creates error log tables that you can query:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- View recent errors from the current session\nSELECT * FROM gp_read_error_log('sales_data_logged');\n"})}),"\n",(0,a.jsx)(r.h3,{id:"error-log-table-structure",children:"Error log table structure"}),"\n",(0,a.jsx)(r.p,{children:"The error log contains these columns:"}),"\n",(0,a.jsxs)(r.table,{children:[(0,a.jsx)(r.thead,{children:(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.th,{children:"Column"}),(0,a.jsx)(r.th,{children:"Description"})]})}),(0,a.jsxs)(r.tbody,{children:[(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"cmdtime"})}),(0,a.jsx)(r.td,{children:"Timestamp when the error occurred"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"relname"})}),(0,a.jsx)(r.td,{children:"Name of the external table"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"filename"})}),(0,a.jsx)(r.td,{children:"Source file containing the error"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"linenum"})}),(0,a.jsx)(r.td,{children:"Line number in the source file"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"bytenum"})}),(0,a.jsx)(r.td,{children:"Byte position in the source file"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"errmsg"})}),(0,a.jsx)(r.td,{children:"Error message description"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"rawdata"})}),(0,a.jsx)(r.td,{children:"Raw data that caused the error"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"rawbytes"})}),(0,a.jsx)(r.td,{children:"Raw bytes of the problematic data"})]})]})]}),"\n",(0,a.jsx)(r.h3,{id:"example-error-analysis",children:"Example error analysis"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Find the most common error types\nSELECT errmsg, COUNT(*) as error_count\nFROM gp_read_error_log('sales_data_logged')\nGROUP BY errmsg\nORDER BY error_count DESC;\n\n-- View specific error details\nSELECT cmdtime, filename, linenum, errmsg, rawdata\nFROM gp_read_error_log('sales_data_logged')\nWHERE errmsg LIKE '%invalid input syntax%'\nORDER BY cmdtime DESC;\n"})}),"\n",(0,a.jsx)(r.h2,{id:"common-data-errors-and-solutions",children:"Common data errors and solutions"}),"\n",(0,a.jsx)(r.h3,{id:"data-type-conversion-errors",children:"Data type conversion errors"}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Error"}),": ",(0,a.jsx)(r.code,{children:"invalid input syntax for type numeric"}),"\n",(0,a.jsx)(r.strong,{children:"Cause"}),": Non-numeric data in numeric columns\n",(0,a.jsx)(r.strong,{children:"Solution"}),": Clean data or use text columns with post-processing"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Original problematic data: \"N/A\" in amount column\n-- Solution: Use text type and handle conversion later\nCREATE EXTERNAL TABLE sales_flexible (\n    transaction_id int,\n    product_name text,\n    sale_date text,  -- Use text for flexible parsing\n    amount text      -- Use text to handle \"N/A\" values\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nLOG ERRORS SEGMENT REJECT LIMIT 10 PERCENT;\n"})}),"\n",(0,a.jsx)(r.h3,{id:"date-format-issues",children:"Date format issues"}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Error"}),": ",(0,a.jsx)(r.code,{children:"invalid input syntax for type date"}),(0,a.jsx)(r.br,{}),"\n",(0,a.jsx)(r.strong,{children:"Cause"}),": Inconsistent date formats\n",(0,a.jsx)(r.strong,{children:"Solution"}),": Standardize date formats or use flexible parsing"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Handle multiple date formats in post-processing\nSELECT \n    transaction_id,\n    product_name,\n    CASE \n        WHEN sale_date ~ '^\\d{4}-\\d{2}-\\d{2}$' THEN sale_date::date\n        WHEN sale_date ~ '^\\d{2}/\\d{2}/\\d{4}$' THEN to_date(sale_date, 'MM/DD/YYYY')\n        ELSE NULL\n    END as parsed_date,\n    amount::decimal(10,2)\nFROM sales_flexible\nWHERE sale_date IS NOT NULL;\n"})}),"\n",(0,a.jsx)(r.h3,{id:"miss-or-extra-columns",children:"Miss or extra columns"}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Error"}),": ",(0,a.jsx)(r.code,{children:"extra data after last expected column"}),(0,a.jsx)(r.br,{}),"\n",(0,a.jsx)(r.strong,{children:"Cause"}),": Inconsistent number of columns\n",(0,a.jsx)(r.strong,{children:"Solution"}),": Use more flexible table definition"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Add extra optional columns to handle variable column counts\nCREATE EXTERNAL TABLE flexible_sales (\n    transaction_id int,\n    product_name text,\n    sale_date date,\n    amount decimal(10,2),\n    extra_field1 text,  -- Optional fields\n    extra_field2 text,\n    extra_field3 text\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV' (HEADER)\nLOG ERRORS SEGMENT REJECT LIMIT 20 PERCENT;\n"})}),"\n",(0,a.jsx)(r.h3,{id:"character-encoding-issues",children:"Character encoding issues"}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Error"}),": ",(0,a.jsx)(r.code,{children:"invalid byte sequence"}),"\n",(0,a.jsx)(r.strong,{children:"Cause"}),": Character encoding mismatch\n",(0,a.jsx)(r.strong,{children:"Solution"}),": Specify correct encoding"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE encoded_data (\n    id int,\n    description text\n)\nLOCATION ('gpfdist://etl-server:8081/data/*.txt')\nFORMAT 'TEXT' (DELIMITER '|')\nENCODING 'LATIN1'  -- Specify encoding\nLOG ERRORS SEGMENT REJECT LIMIT 5 PERCENT;\n"})}),"\n",(0,a.jsx)(r.h2,{id:"error-handling-strategies",children:"Error handling strategies"}),"\n",(0,a.jsx)(r.h3,{id:"two-phase-loading",children:"Two-phase loading"}),"\n",(0,a.jsx)(r.p,{children:"Use a staging approach for complex data cleaning:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Phase 1: Load into staging table with flexible types\nCREATE EXTERNAL TABLE sales_staging (\n    transaction_id text,\n    product_name text,\n    sale_date text,\n    amount text,\n    raw_line text  -- Store entire row for complex cases\n)\nLOCATION ('gpfdist://etl-server:8081/sales/*.csv')\nFORMAT 'CSV'\nLOG ERRORS SEGMENT REJECT LIMIT 20 PERCENT;\n\n-- Phase 2: Clean and insert into final table\nINSERT INTO sales_final (transaction_id, product_name, sale_date, amount)\nSELECT \n    transaction_id::int,\n    product_name,\n    sale_date::date,\n    amount::decimal(10,2)\nFROM sales_staging\nWHERE transaction_id ~ '^\\d+$'  -- Validate numeric ID\n  AND sale_date ~ '^\\d{4}-\\d{2}-\\d{2}$'  -- Validate date format\n  AND amount ~ '^\\d+\\.?\\d*$';  -- Validate amount format\n"})}),"\n",(0,a.jsx)(r.h3,{id:"error-threshold-monitoring",children:"Error threshold monitoring"}),"\n",(0,a.jsx)(r.p,{children:"Set up monitoring for error rates:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-sql",children:"-- Function to check error rate\nCREATE OR REPLACE FUNCTION check_error_rate(table_name text, threshold_percent numeric)\nRETURNS boolean AS $$\nDECLARE\n    error_count int;\n    total_count int;\n    error_rate numeric;\nBEGIN\n    SELECT COUNT(*) INTO error_count FROM gp_read_error_log(table_name);\n    \n    -- Estimate total processed rows (depends on your tracking method)\n    SELECT reltuples INTO total_count FROM pg_class WHERE relname = table_name;\n    \n    IF total_count > 0 THEN\n        error_rate := (error_count::numeric / total_count) * 100;\n        RETURN error_rate <= threshold_percent;\n    END IF;\n    \n    RETURN true;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT check_error_rate('sales_data_logged', 5.0);  -- Check if error rate is under 5%\n"})}),"\n",(0,a.jsx)(r.h2,{id:"best-practices",children:"Best practices"}),"\n",(0,a.jsx)(r.h3,{id:"design-for-errors",children:"Design for errors"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Begin with permissive table definitions using text data types and generous error limits to understand the full scope of data quality issues before implementing strict validation rules."}),"\n",(0,a.jsx)(r.li,{children:"Implement data validation incrementally by gradually adding constraints and type conversions as you gain confidence in data quality and identify patterns in the error logs."}),"\n",(0,a.jsx)(r.li,{children:"Establish regular monitoring and review processes for error logs to identify systematic data quality issues and trends that may indicate problems with source systems or data processing pipelines."}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"error-limit-guidelines",children:"Error limit guidelines"}),"\n",(0,a.jsxs)(r.table,{children:[(0,a.jsx)(r.thead,{children:(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.th,{children:"Data Quality"}),(0,a.jsx)(r.th,{children:"Suggested Reject Limit"}),(0,a.jsx)(r.th,{children:"Use Case"})]})}),(0,a.jsxs)(r.tbody,{children:[(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"High quality"}),(0,a.jsx)(r.td,{children:"1-10 rows"}),(0,a.jsx)(r.td,{children:"Production systems"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"Medium quality"}),(0,a.jsx)(r.td,{children:"1-5%"}),(0,a.jsx)(r.td,{children:"Development/testing"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"Poor quality"}),(0,a.jsx)(r.td,{children:"10-20%"}),(0,a.jsx)(r.td,{children:"Initial data exploration"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"Unknown quality"}),(0,a.jsx)(r.td,{children:"50%"}),(0,a.jsx)(r.td,{children:"Data discovery phase"})]})]})]}),"\n",(0,a.jsx)(r.h3,{id:"operational-procedures",children:"Operational procedures"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Establish a regular schedule for cleaning up old error logs to prevent excessive storage consumption and maintain system performance."}),"\n",(0,a.jsx)(r.li,{children:"Configure monitoring systems to alert when error rates exceed predefined thresholds, enabling quick response to data quality issues."}),"\n",(0,a.jsx)(r.li,{children:"Maintain communication channels with data providers to share error patterns and collaborate on improving source data quality."}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"performance-considerations",children:"Performance considerations"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Error logging functionality introduces some computational overhead during data loading operations, so consider this impact when processing large datasets."}),"\n",(0,a.jsx)(r.li,{children:"Setting higher segment reject limits allows more rows to be processed before the operation fails, but this also means more resources are consumed analyzing problematic data."}),"\n",(0,a.jsx)(r.li,{children:"Error log tables can grow significantly in production environments, so implement monitoring to track storage usage and prevent disk space issues."}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(r.h3,{id:"high-error-rates",children:"High error rates"}),"\n",(0,a.jsx)(r.p,{children:"If you are experiencing high error rates:"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Analyze the error log entries to identify patterns or systematic issues in your data, such as consistent formatting problems or missing values in specific columns."}),"\n",(0,a.jsx)(r.li,{children:"Work with data providers to verify the quality and consistency of source data, including checking for recent changes in data formats or processing."}),"\n",(0,a.jsx)(r.li,{children:"Carefully review your external table definitions to ensure that column data types, delimiters, and format specifications accurately match the actual data structure."}),"\n",(0,a.jsx)(r.li,{children:"Start troubleshooting with small data samples to isolate issues quickly before processing larger datasets."}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"performance-issues",children:"Performance issues"}),"\n",(0,a.jsx)(r.p,{children:"If error handling is impacting performance:"}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsx)(r.li,{children:"Fine-tune your segment reject limits to balance between fault tolerance and processing efficiency, avoiding unnecessarily high thresholds that waste resources."}),"\n",(0,a.jsx)(r.li,{children:"For complex data with known quality issues, consider implementing a two-phase loading process using staging tables with flexible data types."}),"\n",(0,a.jsx)(r.li,{children:"Break large data loads into smaller, manageable batches to reduce memory pressure and improve error isolation."}),"\n",(0,a.jsx)(r.li,{children:"Continuously monitor system resources including memory usage, disk I/O, and storage consumption during data loading operations."}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"learn-more",children:"Learn more"}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:(0,a.jsx)(r.a,{href:"/docs/next/data-loading/load-data-using-gpfdist",children:"Load Data Using gpfdist"})}),"\n",(0,a.jsx)(r.li,{children:(0,a.jsx)(r.a,{href:"/docs/next/data-loading/load-data-using-copy",children:"Load Data Using COPY"})}),"\n"]})]})}function h(e={}){const{wrapper:r}={...(0,s.a)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},11151:(e,r,n)=>{n.d(r,{Z:()=>o,a:()=>t});var a=n(67294);const s={},i=a.createContext(s);function t(e){const r=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:r},e.children)}}}]);